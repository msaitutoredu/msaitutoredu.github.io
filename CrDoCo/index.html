<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
  <meta name="author" content="jasonlai">
  <title>CrDoCo</title>

  <!-- CSS  -->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link href="css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/font-awesome.min.css" rel="stylesheet">

  <!--<meta property="og:image" content="http://gph.is/2oZQz8h" />-->
</head>
<body>
  
  <div class="navbar-fixed">
    <nav class="grey darken-4" role="navigation">
      <div class="nav-wrapper container"><a id="logo-container" href="#" class="brand-logo"></a>
        <ul class="left hide-on-med-and-down">
          <li><a class="nav-item waves-effect waves-light" href="#home">Home</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#abstract">Abstract</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#paper">Paper</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#citation">Citation</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#code">Code</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#network">Network</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#reference">References</a></li>
        </ul>
        <a href="#" data-activates="nav-mobile" class="button-collapse"><i class="material-icons">menu</i></a>
      </div>
    </nav>
  </div>
  

  <div class="section no-pad-bot" id="index-banner">
    <div class="container scrollspy" id="home">
        <h4 class="header center black-text">CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency</h4>

      <br>

      <div class="row center">
        <h5 class="header col m3 s12">
          <div class="author"><a href="https://yunchunchen.github.io" target="blank">Yun-Chun Chen</a></div>
          <div class="school"><a href="https://www.sinica.edu.tw/en" target="blank">Academia Sinica</a></div>
          <div class="school"><a href="http://www.ntu.edu.tw/english/" target="blank">National Taiwan University</a></div>
        </h5>

        <h5 class="header col m3 s12">
          <div class="author"><a href="https://www.citi.sinica.edu.tw/pages/yylin/" target="blank">Yen-Yu Lin</a></div>
          <div class="school"><a href="https://www.sinica.edu.tw/en" target="blank">Academia Sinica</a></div>
        </h5>

        <h5 class="header col m3 s12">
          <div class="author"><a href="http://faculty.ucmerced.edu/mhyang/" target="blank">Ming-Hsuan Yang</a></div>
          <div class="school"><a href="http://www.ucmerced.edu/" target="blank">University of California, Merced</a></div>
          <div class="school"><a href="" target="blank">Google</a></div>
        </h5>
        
        <h5 class="header col m3 s12">
          <div class="author"><a href="https://filebox.ece.vt.edu/~jbhuang/" target="blank">Jia-Bin Huang</a></div>
          <div class="school"><a href="http://www.vt.edu/" target="blank">Virginia Tech</a></div>
        </h5>

      </div>

    </div>
  </div>


  <div class="container">

    <div class="section">

      <!--   Icon Section   -->
      <div class="row center">
        <div class="col l10 offset-l1 s12">
          <a href="img/teaser.png" target="_blank">
          <img class="responsive-img" src="img/teaser.png">
          </a>
        </div>
      </div>

    </div>

    <div class="row section scrollspy" id="abstract">
      <div class="title">Abstract</div>
      <br>
      Unsupervised domain adaptation algorithms aim to transfer the knowledge learned from one domain to another (e.g., synthetic to real images). 
      The adapted representa- tions often do not capture pixel-level domain shifts that are crucial for dense prediction tasks (e.g., semantic segmenta- tion). 
      In this paper, we present a novel pixel-wise adversarial domain adaptation algorithm. 
      By leveraging image-to-image translation methods for data augmentation, our key insight is that while the translated images between domains may differ in styles, their predictions for the task should be consistent.
      We exploit this property and introduce a cross-domain consistency loss that enforces our adapted model to produce consistent predictions. 
      Through extensive experimental results, we show that our method compares favorably against the state-of-the-art on a wide variety of unsupervised domain adaptation tasks.
    </div>


      <div class="row">
      
        <div class="col m6 s12 center">
          <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_CrDoCo_Pixel-Level_Domain_Transfer_With_Cross-Domain_Consistency_CVPR_2019_paper.pdf" target="_blank">
            <img class="responsive-img" src="img/cover.png">
          </a>
          <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_CrDoCo_Pixel-Level_Domain_Transfer_With_Cross-Domain_Consistency_CVPR_2019_paper.pdf" target="_blank">CVPR (CVF)</a>
        </div>

        <div class="col m6 s12 center">
          <a href="https://arxiv.org/abs/2001.03182" target="_blank">
            <img class="responsive-img" src="img/cover.png">
          </a>
          <a href="https://arxiv.org/abs/2001.03182" target="_blank">CVPR (arXiv)</a>
        </div>

      </div>

    </div>


    <div class="row section scrollspy" id="citation">
      <div class="title">Citation</div>
      <p>Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, and Jia-Bin Huang, "CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency", in IEEE Conference on Computer Vision and Pattern Recognition, 2019.</p>
      
      <br>

      <div class="title">BibTex</div>
      <pre>
@inproceedings{CrDoCo,
  author    = {Chen, Yun-Chun and Lin, Yen-Yu and Yang, Ming-Hsuan and Huang, Jia-Bin}, 
  title     = {CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2019}
}
      </pre>

    </div>


    <div class="section row scrollspy" id="code">
      <div class="title">Code</div>
      <div class="row">
          <br>
        <div class="col m4 s12">
          <a href="https://github.com/YunChunChen/CrDoCo-pytorch" target="_blank">PyTorch</a>
        </div>
      </div>
    </div>


    <br>

    <div class="section row scrollspy" id="network">
      <div class="title">CrDoCo</div>
      <br>
      <div class="row center">
        <div class="col l11 offset-l1 s12">
          <img class="responsive-img" src="img/Model.png">
        </div>
      </div>

    </div>

    <div class="row section scrollspy" id="reference">
      <div class="title">References</div>
      <ul>
        <li>&bull; 
           Ganin et al. <a href="http://proceedings.mlr.press/v37/ganin15.pdf" target="blank">Unsupervised Domain Adaptation by Backpropagation</a>. In ICML, 2015.
        </li>
        <li>&bull; 
           Long et al. <a href="https://arxiv.org/pdf/1502.02791.pdf" target="blank">Learning Transferable Features with Deep Adaptation Networks</a>. In ICML, 2015.
        </li>
        <li>&bull; 
           Tzeng et al. <a href="https://arxiv.org/abs/1510.02192" target="blank">Simultaneous Deep Transfer Across Domains and Tasks</a>. In ICCV, 2015.
        </li>
        <li>&bull; 
           Sun et al. <a href="https://arxiv.org/abs/1607.01719" target="blank">Deep CORAL: Correlation Alignment for Deep Domain Adaptation</a>. In ECCV, 2016.
        </li>
        <li>&bull; 
           Ganin et al. <a href="https://arxiv.org/abs/1505.07818" target="blank">Domain-Adversarial Training of Neural Networks</a>. JMLR, 2016.
        </li>
        <li>&bull; 
           Long et al. <a href="https://arxiv.org/abs/1602.04433" target="blank">Unsupervised Domain Adaptation with Residual Transfer Networks</a>. In NIPS, 2016.
        </li>
        <li>&bull; 
          Hoffman et al. <a href="https://arxiv.org/abs/1612.02649" target="blank">FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation</a>. arXiv, 2016.
        </li>
        <li>&bull; 
          Tobin et al. <a href="https://arxiv.org/abs/1703.06907" target="blank">Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World</a>. In IROS, 2017.
        </li>
        <li>&bull; 
           Bousmalis et al. <a href="https://arxiv.org/abs/1612.05424" target="blank">Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks</a>. In CVPR, 2017.
        </li>
        <li>&bull; 
           Shrivastava et al. <a href="https://arxiv.org/abs/1612.07828" target="blank">Learning from Simulated and Unsupervised Images through Adversarial Training</a>. In CVPR, 2017.
        </li>
        <li>&bull; 
           Tzeng et al. <a href="https://arxiv.org/abs/1702.05464" target="blank">Adversarial Discriminative Domain Adaptation</a>. In CVPR, 2017.
        </li>
        <li>&bull; 
           Chen et al. <a href="https://arxiv.org/abs/1704.08509" target="blank">No More Discrimination: Cross City Adaptation of Road Scene Segmenters</a>. In ICCV,  2017.
        </li>
        <li>&bull; 
           Zhang et al. <a href="https://arxiv.org/abs/1707.09465" target="blank">Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes</a>. In ICCV, 2017.
        </li>
        <li>&bull; 
           Dundar et al. <a href="https://arxiv.org/abs/1807.09384" target="blank">Domain Stylization: A Strong, Simple Baseline for Synthetic to Real Image Domain Adaptation</a>. arXiv, 2018.
        </li>
        <li>&bull; 
          Hoffman et al. <a href="https://arxiv.org/abs/1711.03213" target="blank">CyCADA: Cycle-Consistent Adversarial Domain Adaptation</a>. In ICML, 2018.
        </li>
        <li>&bull; 
           Tsai et al. <a href="https://arxiv.org/abs/1802.10349" target="blank">Learning to Adapt Structured Output Space for Semantic Segmentation</a>. In CVPR, 2018.
        </li>
        <li>&bull; 
           Sankaranarayanan et al. <a href="https://arxiv.org/abs/1711.06969" target="blank">Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation</a>. In CVPR, 2018.
        </li>
        <li>&bull; 
           Huang et al. <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Haoshuo_Huang_Domain_transfer_through_ECCV_2018_paper.pdf" target="blank"> Domain transfer through deep activation matching</a>. In ECCV, 2018.
        </li>
        <li>&bull; 
           Peng et al. <a href="https://arxiv.org/abs/1806.09755" target="blank">Syn2Real: A New Benchmark forSynthetic-to-Real Visual Domain Adaptation</a>. In ECCV, 2018.
        </li>
        <li>&bull; 
           Sakaridis et al. <a href="https://arxiv.org/abs/1808.01265" target="blank">Model Adaptation with Synthetic and Real Data for Semantic Dense Foggy Scene Understanding</a>. In ECCV, 2018.
        </li>
        <li>&bull; 
           Zheng et al. <a href="https://arxiv.org/abs/1808.01454" target="blank">T2Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth Estimation Tasks</a>. In ECCV, 2018.
        </li>
        <li>&bull; 
           Zou et al. <a href="https://arxiv.org/abs/1810.07911" target="blank">Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training</a>. In ECCV, 2018.
        </li>
      </ul>
    </div>

  </div>


  <!--  Scripts-->
  <script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
  <script src="js/materialize.js"></script>
  <script src="js/init.js"></script>

  </body>
</html>


